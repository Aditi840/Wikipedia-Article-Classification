# -*- coding: utf-8 -*-
"""Untitled13.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/14U912ydiFfVX8UuMn0ZcoqcoLJu75Kl-
"""

!pip install pandas
!pip install wikipedia
!pip install beautifulsoup4
!pip install requests

import pandas as pd
import wikipedia
import re
from bs4 import BeautifulSoup
import requests
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

#url = "https://en.wikipedia.org/wiki/Python_(programming_language)"
#response = requests.get(url)
#html = response.text

#soup = BeautifulSoup(html, features="html.parser")
#lis = soup.find_all('li')


# read data from file
with open("/content/articleDesc.csv", 'r', encoding='utf-8') as f:
    data = f.read()

# split data into rows based on @$@ separator
rows = data.split('\n')

# split each row into columns based on @$@ separator
columns = [row.split('@$@') for row in rows]

# create pandas dataframe
df = pd.DataFrame(columns)

df = df.fillna('unknown')
print(df.isnull())
print(df.isnull().sum())

# set column names
df.columns = ['Article Name', 'Vital Article', 'Level', 'Class', 'Importance', 'Topic', 'Wikiproject']

# create separate dataframes for articles with "FA" and without "FA"
fa_articles = df[df['Class'].str.contains('FA')].sample(n=5000, replace=True)
non_fa_articles = df[~df['Class'].str.contains('FA')].sample(n=5000, replace=True)

# combine the two dataframes
combined_df = pd.concat([fa_articles, non_fa_articles])

# print the combined dataframe
print(combined_df)

features_df = pd.DataFrame()


# loop through each article in the combined dataframe and extract the features
for index, row in combined_df.iterrows():
    article_name = row['Article Name']
    try:
        # extract article length
        page = wiki.page(article_name)
        article_length = len(page.text)
    except:
        article_length = 'unknown'
    
    try:
        # extract number of references
        references = wikipedia.page(article_name).references
        num_references = len(references)
    except:
        num_references = 'unknown'
    # Replace unknown values with 0
    if num_references == 'unknown':
        num_references = 0
    
    try:
        # extract quality of references
        quality_of_references = 'high'
        for ref in references:
            if ref.endswith('.pdf'):
                quality_of_references = 'low'
                break
    except:
        quality_of_references = 'unknown'
    
    try:
        # extract number of links
        links = page.links
        num_links = len(links)
    except:
        num_links = 'unknown'
    
    try:
        # extract number of categories
        categories = page.categories
        num_categories = len(categories)
    except:
        num_categories = 'unknown'       
        
      # add the extracted features to the new dataframe
    features_df = pd.concat([features_df, pd.DataFrame({
        'Article Name': [article_name],
        'Article Length': [article_length],
        'Number of References': [num_references],
        'Quality of References': [quality_of_references],
        'Number of Links': [num_links],
        'Number of Categories': [num_categories]
    })])

# replace unknown values with 0
features_df = features_df.replace('unknown', 0)
features_df['Quality of References'] = features_df['Quality of References'].replace({'high': 1, 'low': 0})
print(features_df.columns)

# create X and y variables
X = features_df.drop('Article Name', axis=1).values

y = [1 if x else 0 for x in combined_df['Class'].str.contains('FA').values]
# split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# create and train a logistic regression model
model = LogisticRegression()
model.fit(X_train, y_train)

# predict the class of the testing set
y_pred = model.predict(X_test)

# evaluate the performance of the model
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print('Accuracy: {:.2f}'.format(accuracy))
print('Precision: {:.2f}'.format(precision))
print('Recall: {:.2f}'.format(recall))
print('F1-score: {:.2f}'.format(f1))